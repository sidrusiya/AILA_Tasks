{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task2_preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjQUyDeMcbtjlFdaV8ZV8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidrusiya/AILA_Tasks/blob/main/Task2_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0GuVknUrQim"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCNj6QxArXbU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiVOZ-Cdrbqs"
      },
      "source": [
        "dataset = open('/content/drive/MyDrive/Task2/task2_headnotes.json', 'r')\n",
        "data = json.load(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gISA001wrgVo"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogC0qd_-rjOM"
      },
      "source": [
        "yz = pd.DataFrame.from_dict(data, orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j6IQJNgrpYQ"
      },
      "source": [
        "dataset1 = open('/content/drive/MyDrive/Task2/task2_judgements.json', 'r')\n",
        "data1 = json.load(dataset1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSdnixWervB4"
      },
      "source": [
        "yz1 = pd.DataFrame.from_dict(data1, orient='index')\n",
        "yz1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9oWImlfry1w"
      },
      "source": [
        "df = pd.DataFrame(columns=['judge_id', 'id', 'sentence', 'relevance'])\n",
        "for i in range(2545):\n",
        "  for j in range(500):\n",
        "    if yz1[i][j] is not None:\n",
        "      df = df.append({'judge_id': j+1, 'id': yz1[i][j]['id'], 'sentence': yz1[i][j]['sentence'], 'relevance': yz1[i][j]['relevance']}, ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNpRFO-Vr88n"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0fNZpdUsA5q"
      },
      "source": [
        "import seaborn as sns\n",
        "import gensim\n",
        "nltk.download('all')\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "!pip3 install tensorflow_text\n",
        "import tensorflow_text\n",
        "!pip install -q tf-models-official\n",
        "!pip install -q -U tensorflow-text\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as tf_tex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv6G7DFqsJMW"
      },
      "source": [
        "# Remove Excess [Size:: ] from EoS\n",
        "def remove_size(text):\n",
        "\tresult = re.search(\"\\[size\", text)\n",
        "\tif result is not None:\n",
        "\t\ttext = text[:result.start()]\n",
        "\treturn text.strip()\n",
        "\n",
        "months = ['january', 'jan', 'feb', 'mar', 'apr', 'aug', 'sept', 'oct', 'nov', 'dec', 'feburary', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'on']\n",
        "def remove_months(text):\n",
        "    text = text.split()\n",
        "    text = [word for word in text if word.lower() not in set(months)]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "def remove_nums(text):\n",
        "    result = re.sub(r'[0-9+]', '', text)\n",
        "    return result\n",
        "\n",
        "def remove_comma(text):\n",
        "    result = re.sub(r',', '', text)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goYPfBSIsO2I"
      },
      "source": [
        "import sys\n",
        "text = []\n",
        "lem_text = []\n",
        "for i in df.index:\n",
        "    df['sentence'][i] = remove_size(df['sentence'][i])\n",
        "    df['sentence'][i] = remove_months(df['sentence'][i])\n",
        "    df['sentence'][i] = remove_nums(df['sentence'][i])\n",
        "    df['sentence'][i] = remove_comma(df['sentence'][i])\n",
        "    temp = df['sentence'][i].lower().split()\n",
        "    clean = [word for word in temp if word not in stopwords.words('english')]\n",
        "    clean = \" \".join(clean)\n",
        "    text.append(clean)\n",
        "    lem_text.append(clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUbvNoEBsT9O"
      },
      "source": [
        "for i in range(len(text)):\n",
        "    text[i] = text[i].split()\n",
        "\n",
        "for i in range(len(lem_text)):\n",
        "    lem_text[i] = lem_text[i].split()\n",
        "\n",
        "from nltk import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "for i in range(len(text)):\n",
        "    text[i] = [st.stem(word) for word in text[i]]\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "  \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for i in range(len(lem_text)):\n",
        "    lem_text[i] = [lemmatizer.lemmatize(word) for word in lem_text[i]]\n",
        "\n",
        "for i in range(len(text)):\n",
        "    text[i] = \" \".join(text[i])\n",
        "\n",
        "text[0]\n",
        "\n",
        "for i in range(len(lem_text)):\n",
        "    lem_text[i] = \" \".join(lem_text[i])\n",
        "\n",
        "lem_text[0]\n",
        "\n",
        "df['lem_text'] = np.array(lem_text)\n",
        "df['stem_text'] = np.array(text)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3S5pWJCsdA6"
      },
      "source": [
        "df.relevance.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGmgTKR6suus"
      },
      "source": [
        "df.to_csv('nit_agartala_nlp_team_9.tsv', sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}